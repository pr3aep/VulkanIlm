# VulkanIlm â€“ Run LLaMA on Fire

**VulkanIlm** (from _"Vulkan"_ ğŸ”¥ and _"Ilm"_ ğŸ“š meaning "knowledge" in Urdu/Arabic) is a lightweight, Pythonic wrapper for running **LLaMA models** on legacy GPUs using the **Vulkan backend** from [`llama.cpp`](https://github.com/ggerganov/llama.cpp).

ğŸš€ Designed for developers with older AMD GPUs (like the RX 580), `VulkanIlm` enables blazing-fast local LLM inference without needing CUDA or ROCm.

---

## ğŸ§  Why VulkanIlm?

> Not everyone has a CUDA-capable GPU. Not everyone should need one to use AI.

`VulkanIlm` is:
- âœ… **Python-first**: No C++/CMake experience required
- ğŸ§µ Built on top of [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python) â€“ but extended to support **Vulkan**
- ğŸ”¥ **GPU-accelerated** inference even on older AMD cards
- ğŸ“¦ Easy to install, use, and deploy in your own AI apps

---

## ğŸ“¦ Installation

> **Prerequisites**:
> - Vulkan-capable GPU (e.g. AMD RX 580)
> - Vulkan drivers installed
> - Python 3.8+

```bash
pip install vulkanilm
````

> If Vulkan backend isn't detected, you can manually build `llama.cpp` with Vulkan support and point to it. (See [Build Instructions](#ï¸-vulkan-build-instructions))

---

## ğŸ§‘â€ğŸ’» Example Usage

```python
from vulkanilm import VulkanLlama

llm = VulkanLlama(model_path="models/mistral.gguf", n_gpu_layers=100)

response = llm.chat("Explain the term 'ilm' in AI context.")
print(response)
```

> â˜‘ï¸ Uses GPU if available
> â˜‘ï¸ Falls back gracefully if not
> â˜‘ï¸ Streamlined `chat()` interface

---

## âš™ï¸ Vulkan Build Instructions (If Needed)

If your system doesn't detect Vulkan automatically:

1. Clone `llama.cpp` with Vulkan enabled:

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
LLAMA_VULKAN=1 make libllama.so
```

2. In your Python code, load the shared library manually:

```python
llm = VulkanLlama(
    model_path="models/mistral.gguf",
    lib_path="path/to/llama.cpp/build/libllama.so"
)
```

---

## ğŸŒ Features

* âœ¨ `chat()` and `generate()` APIs
* ğŸ’¬ Supports system prompts and role-based formatting
* ğŸ”¥ Vulkan-backed token sampling (greedy, top-k/top-p)
* ğŸ§  Custom `llama_context_params` options
* ğŸ“œ Streaming token generation
* ğŸ§¹ Memory-managed lifecycle (calls `llama_free()`)

---

## ğŸ“ Roadmap

* [x] LLaMA loading with Vulkan shared lib
* [x] Simple Python wrapper with `llama-cpp-python`
* [ ] Model quantization CLI
* [ ] CLI runner: `vulkanilm chat -m model.gguf`
* [ ] Web UI interface with Gradio

---

## ğŸ§¾ Meaning Behind the Name

In South Asian and Islamic culture, **"Ilm" (Ø¹Ù„Ù…)** means *knowledge*, wisdom, and divine understanding.
Combined with **Vulkan**, a high-performance GPU API, this project stands for **accessible, local AI power** â€” *knowledge on fire*.

---

## ğŸ™Œ Contributing

PRs are welcome! Fork this repo, open issues, and help bring fast, GPU-backed LLMs to everyone.

---

## ğŸ“„ License

MIT License â€“ free to use, modify, and share.

---

> *Built with ğŸ”¥ by Talha | Running local LLMs on fire for everyone, everywhere.*

---

## ğŸ§© GitHub Topics (Add These for Visibility)

```
llama-cpp-python, vulkan, llm-inference, python-wrapper, amd-gpu, open-source-llm, gguf, fastai, legacy-gpus, desi-tech
```

```

---

Let me know if you want the:
- ğŸ§  **Copilot Space tagline + instructions**
- ğŸ“¦ `pyproject.toml` + build instructions
- ğŸ–¼ï¸ SVG logo for favicon or header badge
- ğŸ“˜ Dev guide with internal structure and extension hooks

Youâ€™ve got something powerful brewing here â€” letâ€™s package it for the world ğŸŒ
```
