# VulkanIlm â€“ Run LLaMA on Fire

**VulkanIlm** (from _"Vulkan"_ ðŸ”¥ and _"Ilm"_ ðŸ“š meaning "knowledge" in Urdu/Arabic) is a lightweight, Pythonic wrapper for running **LLaMA models** on legacy GPUs using the **Vulkan backend** from [`llama.cpp`](https://github.com/ggerganov/llama.cpp).

Designed for developers with older AMD/Intel GPUs, `VulkanIlm` enables blazing-fast local LLM inference without needing CUDA or ROCm.

---

## ðŸ§  Why VulkanIlm?

> Not everyone has a CUDA-capable GPU. Not everyone should need one to use AI.

`VulkanIlm` is:
- âœ… **Python-first**: No C++/CMake experience required
- ðŸ§µ Built on top of [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python) â€“ but extended to support **Vulkan**
- ðŸ”¥ **GPU-accelerated** inference even on older AMD cards
- ðŸ“¦ Easy to install, use, and deploy in your own AI apps

---

## ðŸ“¦ Installation

> **Prerequisites**:
> - Vulkan-capable GPU (e.g. AMD RX 580)
> - Vulkan drivers installed
> - Python 3.8+

```bash
pip install vulkanilm
````

> If Vulkan backend isn't detected, you can manually build `llama.cpp` with Vulkan support and point to it. (See [Build Instructions](#ï¸-vulkan-build-instructions))

---

## ðŸ§‘â€ðŸ’» Example Usage

```python
from vulkanilm import VulkanLlama

llm = VulkanLlama(model_path="models/mistral.gguf", n_gpu_layers=100)

response = llm.chat("Explain the term 'ilm' in AI context.")
print(response)
```

> â˜‘ï¸ Uses GPU if available
> â˜‘ï¸ Falls back gracefully if not
> â˜‘ï¸ Streamlined `chat()` interface

---

## âš™ï¸ Vulkan Build Instructions (If Needed)

If your system doesn't detect Vulkan automatically:

1. Clone `llama.cpp` with Vulkan enabled:

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
LLAMA_VULKAN=1 make libllama.so
```

2. In your Python code, load the shared library manually:

```python
llm = VulkanLlama(
    model_path="models/mistral.gguf",
    lib_path="path/to/llama.cpp/build/libllama.so"
)
```

---

## ðŸŒ Features

* âœ¨ `chat()` and `generate()` APIs
* ðŸ’¬ Supports system prompts and role-based formatting
* ðŸ”¥ Vulkan-backed token sampling (greedy, top-k/top-p)
* ðŸ§  Custom `llama_context_params` options
* ðŸ“œ Streaming token generation
* ðŸ§¹ Memory-managed lifecycle (calls `llama_free()`)

---

## ðŸ“ Roadmap

* [x] LLaMA loading with Vulkan shared lib
* [x] Simple Python wrapper with `llama-cpp-python`
* [ ] Model quantization CLI
* [ ] CLI runner: `vulkanilm chat -m model.gguf`
* [ ] Web UI interface with Gradio

---

## ðŸ§¾ Meaning Behind the Name

In South Asian and Islamic culture, **"Ilm" (Ø¹Ù„Ù…)** means *knowledge*, wisdom, and divine understanding.
Combined with **Vulkan**, a high-performance GPU API, this project stands for **accessible, local AI power** â€” *knowledge on fire*.

---

## ðŸ™Œ Contributing

PRs are welcome! Fork this repo, open issues, and help bring fast, GPU-backed LLMs to everyone.

---

## ðŸ“„ License

MIT License â€“ free to use, modify, and share.

---

> *Built with ðŸ”¥ by Talha | Running local LLMs on fire for everyone, everywhere.*
